{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409d0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d656f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.25, 0.25, 0.25])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd8eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbb968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "NUM_CLASSES = 38 # 37 + 1 blank\n",
    "CTC_LABELS = [\n",
    "    \"<BLANK>\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"-\", \n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \n",
    "    \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"\n",
    "]\n",
    "COCO_TO_CTC = {\n",
    "    1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10,\n",
    "    11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16,\n",
    "    18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24,\n",
    "    26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32,\n",
    "    34: 33, 35: 34, 36: 35, 37: 36, 38: 37\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663584a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class plate_OCR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.features = nn.Sequential( #bruh it was pooling too much\n",
    "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
    "            resnet.layer1, resnet.layer2\n",
    "        ) \n",
    "        #self.dimension_reduction = nn.Linear(1024, 512)\n",
    "        self.rnn = nn.LSTM(input_size=512, hidden_size=128, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.classify = nn.Linear(128 * 2, NUM_CLASSES) \n",
    "        self.height_conv = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(16, 1), stride=(1, 1), padding=0, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.features(x) \n",
    "        x = self.relu(self.height_conv(x)) \n",
    "        x = x.squeeze(2) \n",
    "        x = x.permute(0, 2, 1) \n",
    "        #print(x.shape)\n",
    "        #x = self.dimension_reduction(x)\n",
    "        #print(x.shape)\n",
    "        rnn, _ = self.rnn(x)\n",
    "        result = self.classify(rnn)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3ad363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(result):\n",
    "    pred = result.argmax(-1).squeeze(0).tolist()\n",
    "    prev = -1\n",
    "    output = []\n",
    "    for p in pred:\n",
    "        if p != prev and p != len(CHARS):\n",
    "            output.append(CHARS[p])\n",
    "        prev = p\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2ceb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from number_coco import license_coco\n",
    "from number_coco import license_collate\n",
    "from torch.utils.data import DataLoader\n",
    "''' for some reason this line wouldn't work here so reput at the top\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "])\n",
    "'''\n",
    "X_train = \"../data/license_numbers/train/images\"\n",
    "y_train = \"../data/license_numbers/train/annotations.json\"\n",
    "\n",
    "train_dataset = license_coco(root=X_train, ann_file=y_train, transforms=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True, collate_fn=license_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6993cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = plate_OCR().to(device)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde43b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94c83683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.2969\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "images, targets, target_lengths = next(testing_train_loader)  # adjust to your loader\n",
    "images = images.to(device)\n",
    "targets = targets.to(device)\n",
    "target_lengths = target_lengths.to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = model(images) # [B, T, C]\n",
    "log_probability = F.log_softmax(outputs, dim=2)\n",
    "log_probability = log_probability.permute(1, 0, 2) # [T, B, C]\n",
    "\n",
    "# Input lengths: full length for each sequence\n",
    "input_lengths = torch.full(\n",
    "    size=(outputs.size(0),),       # B\n",
    "    fill_value=outputs.size(1),    # T\n",
    "    dtype=torch.long\n",
    ").to(device)\n",
    "\n",
    "assert all(input_lengths >= target_lengths), \"Target sequence too long for CTC\"\n",
    "\n",
    "# Loss computation\n",
    "loss = ctc_loss(log_probability, targets, input_lengths, target_lengths)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward and optimize\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f599837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0919e+00, -2.8863e-01,  5.2536e-02, -1.6744e-01, -6.1107e-01,\n",
      "          1.2277e-01, -2.6832e-01, -8.1728e-02, -1.7669e-02,  1.3442e-01,\n",
      "         -1.2289e-01, -1.0407e-01, -3.2711e-01, -5.0249e-01, -6.6734e-02,\n",
      "         -2.2979e-01, -3.0328e-01,  5.3204e-02, -3.1642e-01, -2.0495e-01,\n",
      "         -1.8913e-01, -2.3046e-01, -1.9388e-01,  5.3737e-02, -5.8599e-02,\n",
      "         -3.4637e-01, -3.9886e-02, -4.8267e-02, -1.3595e-01, -2.2445e-01,\n",
      "         -3.5550e-01, -1.7549e-01, -9.6386e-02, -3.0394e-01, -1.4007e-01,\n",
      "         -2.1390e-01, -1.4975e-01, -2.5941e-01],\n",
      "        [ 2.5701e+00, -3.5723e-01,  9.5379e-02, -1.3914e-01, -6.8035e-01,\n",
      "          1.2131e-01, -2.8729e-01, -9.4315e-02, -1.3110e-02,  2.2055e-01,\n",
      "         -1.9837e-01, -6.4635e-02, -3.3668e-01, -5.9638e-01, -7.7981e-02,\n",
      "         -2.9158e-01, -3.8386e-01,  1.3086e-01, -3.0603e-01, -2.2441e-01,\n",
      "         -3.4260e-01, -3.1012e-01, -2.4930e-01,  2.1154e-02, -5.2714e-02,\n",
      "         -4.2790e-01, -4.8737e-02, -2.2328e-02, -1.2636e-01, -3.0258e-01,\n",
      "         -4.4734e-01, -2.3335e-01, -1.4130e-01, -3.3047e-01, -1.8073e-01,\n",
      "         -2.9417e-01, -1.8040e-01, -3.3249e-01],\n",
      "        [ 2.8077e+00, -3.9421e-01,  1.2979e-01, -1.2039e-01, -7.0606e-01,\n",
      "          1.2262e-01, -2.7399e-01, -9.0914e-02, -1.1033e-02,  2.7916e-01,\n",
      "         -2.4005e-01, -3.4628e-02, -3.4702e-01, -6.4532e-01, -8.7067e-02,\n",
      "         -3.2580e-01, -4.1379e-01,  1.8527e-01, -2.9298e-01, -2.2928e-01,\n",
      "         -4.2958e-01, -3.5269e-01, -2.7805e-01,  3.3550e-03, -5.2874e-02,\n",
      "         -4.6074e-01, -6.9893e-02, -2.0373e-02, -1.2720e-01, -3.4682e-01,\n",
      "         -5.0258e-01, -2.7685e-01, -1.7373e-01, -3.5049e-01, -2.0310e-01,\n",
      "         -3.4634e-01, -1.9128e-01, -3.6374e-01],\n",
      "        [ 2.9396e+00, -4.1115e-01,  1.4563e-01, -1.1422e-01, -7.1603e-01,\n",
      "          1.2212e-01, -2.5813e-01, -9.2914e-02, -4.4565e-03,  3.1644e-01,\n",
      "         -2.5772e-01, -2.0613e-02, -3.5105e-01, -6.7881e-01, -8.6351e-02,\n",
      "         -3.4776e-01, -4.3550e-01,  2.1456e-01, -2.8074e-01, -2.3054e-01,\n",
      "         -4.7246e-01, -3.7041e-01, -2.9199e-01,  3.7716e-03, -5.4974e-02,\n",
      "         -4.7518e-01, -8.2429e-02, -2.0737e-02, -1.3568e-01, -3.7608e-01,\n",
      "         -5.3491e-01, -3.0659e-01, -1.9466e-01, -3.6027e-01, -2.0552e-01,\n",
      "         -3.8357e-01, -2.0604e-01, -3.8416e-01],\n",
      "        [ 3.0165e+00, -4.2382e-01,  1.5564e-01, -1.0938e-01, -7.1708e-01,\n",
      "          1.2153e-01, -2.4387e-01, -8.1854e-02, -1.5060e-03,  3.3643e-01,\n",
      "         -2.6971e-01, -1.2332e-02, -3.5242e-01, -6.9995e-01, -8.9468e-02,\n",
      "         -3.5290e-01, -4.4634e-01,  2.2795e-01, -2.7773e-01, -2.3208e-01,\n",
      "         -4.9664e-01, -3.8053e-01, -2.9799e-01,  4.7451e-03, -4.5783e-02,\n",
      "         -4.8268e-01, -8.1604e-02, -2.1195e-02, -1.3560e-01, -3.9037e-01,\n",
      "         -5.5298e-01, -3.2491e-01, -2.0379e-01, -3.6546e-01, -2.1092e-01,\n",
      "         -4.0186e-01, -2.1320e-01, -3.9352e-01],\n",
      "        [ 3.0588e+00, -4.3184e-01,  1.6220e-01, -1.0966e-01, -7.1121e-01,\n",
      "          1.1604e-01, -2.2980e-01, -7.6112e-02, -5.2958e-03,  3.5053e-01,\n",
      "         -2.7070e-01, -2.8481e-03, -3.6219e-01, -7.1060e-01, -8.9702e-02,\n",
      "         -3.5720e-01, -4.5027e-01,  2.3619e-01, -2.7589e-01, -2.2867e-01,\n",
      "         -5.0756e-01, -3.8510e-01, -3.0403e-01,  3.6440e-03, -3.7708e-02,\n",
      "         -4.9058e-01, -7.8657e-02, -2.3938e-02, -1.3509e-01, -4.0259e-01,\n",
      "         -5.5628e-01, -3.3830e-01, -2.1336e-01, -3.6861e-01, -2.1605e-01,\n",
      "         -4.1936e-01, -2.1170e-01, -3.9638e-01],\n",
      "        [ 3.0816e+00, -4.3294e-01,  1.6369e-01, -1.0454e-01, -6.9871e-01,\n",
      "          1.1575e-01, -2.2501e-01, -7.3225e-02, -9.5487e-03,  3.5894e-01,\n",
      "         -2.7918e-01, -1.9472e-03, -3.6742e-01, -7.1148e-01, -8.6927e-02,\n",
      "         -3.6433e-01, -4.5708e-01,  2.4027e-01, -2.6516e-01, -2.3719e-01,\n",
      "         -5.1496e-01, -3.8872e-01, -3.0081e-01,  6.8279e-03, -3.7983e-02,\n",
      "         -4.9755e-01, -8.0406e-02, -2.4179e-02, -1.3943e-01, -4.0993e-01,\n",
      "         -5.6313e-01, -3.5015e-01, -2.2041e-01, -3.7009e-01, -2.1374e-01,\n",
      "         -4.2284e-01, -2.2301e-01, -3.9176e-01],\n",
      "        [ 3.0901e+00, -4.2957e-01,  1.6700e-01, -1.0635e-01, -6.9181e-01,\n",
      "          1.1178e-01, -2.1336e-01, -6.3271e-02, -1.6384e-02,  3.6727e-01,\n",
      "         -2.8193e-01,  5.0197e-03, -3.7496e-01, -7.0494e-01, -7.6931e-02,\n",
      "         -3.6396e-01, -4.4881e-01,  2.3874e-01, -2.6132e-01, -2.2926e-01,\n",
      "         -5.1557e-01, -3.9295e-01, -2.9983e-01,  1.4971e-03, -3.4495e-02,\n",
      "         -5.0816e-01, -7.7292e-02, -2.6611e-02, -1.3707e-01, -4.0262e-01,\n",
      "         -5.6762e-01, -3.5316e-01, -2.3476e-01, -3.7351e-01, -2.1294e-01,\n",
      "         -4.3132e-01, -2.1565e-01, -3.9595e-01],\n",
      "        [ 3.0940e+00, -4.3319e-01,  1.7009e-01, -1.0570e-01, -6.8447e-01,\n",
      "          1.0960e-01, -2.0612e-01, -6.2361e-02, -2.0705e-02,  3.6235e-01,\n",
      "         -2.8783e-01,  6.9516e-03, -3.7771e-01, -7.0231e-01, -7.3200e-02,\n",
      "         -3.5978e-01, -4.5549e-01,  2.3996e-01, -2.5733e-01, -2.2230e-01,\n",
      "         -5.1912e-01, -3.9247e-01, -2.9563e-01,  3.2711e-03, -3.6338e-02,\n",
      "         -5.1184e-01, -6.9449e-02, -2.3422e-02, -1.2938e-01, -4.0908e-01,\n",
      "         -5.7052e-01, -3.5403e-01, -2.3748e-01, -3.7554e-01, -2.1174e-01,\n",
      "         -4.3335e-01, -2.1590e-01, -4.0358e-01],\n",
      "        [ 3.0868e+00, -4.3473e-01,  1.7115e-01, -1.0577e-01, -6.7717e-01,\n",
      "          1.0354e-01, -1.9287e-01, -5.9930e-02, -2.3945e-02,  3.6331e-01,\n",
      "         -2.9115e-01,  1.1259e-02, -3.7318e-01, -7.0136e-01, -6.8306e-02,\n",
      "         -3.5779e-01, -4.5596e-01,  2.4389e-01, -2.5361e-01, -2.2035e-01,\n",
      "         -5.1556e-01, -3.9286e-01, -2.9264e-01, -1.3455e-03, -3.1868e-02,\n",
      "         -5.1090e-01, -6.9277e-02, -2.3600e-02, -1.2869e-01, -4.0860e-01,\n",
      "         -5.7333e-01, -3.5539e-01, -2.3774e-01, -3.7339e-01, -2.1062e-01,\n",
      "         -4.3648e-01, -2.1741e-01, -4.0782e-01],\n",
      "        [ 3.0677e+00, -4.3634e-01,  1.7232e-01, -9.8560e-02, -6.6644e-01,\n",
      "          1.0303e-01, -1.8577e-01, -6.4442e-02, -2.5339e-02,  3.5739e-01,\n",
      "         -2.9143e-01,  1.0272e-02, -3.7113e-01, -7.0040e-01, -6.7252e-02,\n",
      "         -3.5710e-01, -4.5644e-01,  2.4491e-01, -2.4100e-01, -2.2662e-01,\n",
      "         -5.2133e-01, -3.9024e-01, -2.8575e-01, -1.8343e-03, -2.9787e-02,\n",
      "         -5.0933e-01, -6.2773e-02, -1.4449e-02, -1.1972e-01, -4.0656e-01,\n",
      "         -5.7508e-01, -3.5013e-01, -2.3911e-01, -3.7395e-01, -2.0714e-01,\n",
      "         -4.4036e-01, -2.2763e-01, -4.0553e-01],\n",
      "        [ 3.0378e+00, -4.3617e-01,  1.6688e-01, -9.5469e-02, -6.5552e-01,\n",
      "          9.7757e-02, -1.6987e-01, -6.2625e-02, -2.1835e-02,  3.5428e-01,\n",
      "         -2.9301e-01,  1.0793e-02, -3.7297e-01, -6.9124e-01, -5.8508e-02,\n",
      "         -3.5242e-01, -4.5621e-01,  2.5420e-01, -2.3115e-01, -2.2215e-01,\n",
      "         -5.2696e-01, -3.8972e-01, -2.8241e-01, -5.4181e-03, -2.6812e-02,\n",
      "         -5.0886e-01, -5.2600e-02, -4.4415e-03, -1.0508e-01, -3.9573e-01,\n",
      "         -5.8022e-01, -3.4026e-01, -2.4362e-01, -3.7512e-01, -2.0393e-01,\n",
      "         -4.4941e-01, -2.2112e-01, -4.0894e-01],\n",
      "        [ 2.9688e+00, -4.2658e-01,  1.6849e-01, -8.6104e-02, -6.3509e-01,\n",
      "          9.2966e-02, -1.5321e-01, -5.9848e-02, -2.5983e-02,  3.5450e-01,\n",
      "         -2.9135e-01,  2.0379e-02, -3.6839e-01, -6.7376e-01, -5.7825e-02,\n",
      "         -3.5162e-01, -4.4394e-01,  2.6073e-01, -2.1444e-01, -2.2392e-01,\n",
      "         -5.2607e-01, -3.8910e-01, -2.6251e-01, -1.7174e-02, -1.9637e-02,\n",
      "         -4.9207e-01, -5.7851e-02,  8.2510e-03, -9.5319e-02, -3.7274e-01,\n",
      "         -5.7838e-01, -3.3264e-01, -2.3746e-01, -3.6671e-01, -1.9315e-01,\n",
      "         -4.4335e-01, -2.2181e-01, -3.9726e-01],\n",
      "        [ 2.8454e+00, -4.1013e-01,  1.6822e-01, -6.9175e-02, -6.0202e-01,\n",
      "          8.9071e-02, -1.3056e-01, -4.9279e-02, -2.5613e-02,  3.4299e-01,\n",
      "         -2.8715e-01,  2.3302e-02, -3.5446e-01, -6.4711e-01, -5.3825e-02,\n",
      "         -3.4007e-01, -4.3067e-01,  2.6760e-01, -1.8481e-01, -2.2066e-01,\n",
      "         -5.2686e-01, -3.7672e-01, -2.3851e-01, -3.1004e-02, -3.8591e-03,\n",
      "         -4.7460e-01, -5.1333e-02,  2.3644e-02, -7.3931e-02, -3.4459e-01,\n",
      "         -5.6603e-01, -3.1710e-01, -2.3447e-01, -3.5366e-01, -1.8227e-01,\n",
      "         -4.3298e-01, -2.0975e-01, -3.8901e-01],\n",
      "        [ 2.6078e+00, -3.8099e-01,  1.6712e-01, -4.0666e-02, -5.3527e-01,\n",
      "          8.8799e-02, -9.1967e-02, -3.3485e-02, -1.7866e-02,  3.1784e-01,\n",
      "         -2.7873e-01,  3.4443e-02, -3.2232e-01, -6.0062e-01, -4.5866e-02,\n",
      "         -3.3879e-01, -4.0834e-01,  2.8150e-01, -1.2786e-01, -2.0656e-01,\n",
      "         -5.3061e-01, -3.6192e-01, -2.0026e-01, -4.8351e-02,  1.7692e-02,\n",
      "         -4.4929e-01, -3.6999e-02,  4.4307e-02, -3.4599e-02, -3.1443e-01,\n",
      "         -5.3779e-01, -2.9956e-01, -2.2174e-01, -3.2678e-01, -1.5950e-01,\n",
      "         -4.1862e-01, -1.9642e-01, -3.7474e-01],\n",
      "        [ 2.1396e+00, -3.1238e-01,  1.6225e-01,  1.5296e-02, -3.9899e-01,\n",
      "          7.8000e-02, -3.7109e-02,  9.4265e-03, -1.9325e-02,  2.9702e-01,\n",
      "         -2.5841e-01,  5.4090e-02, -2.4642e-01, -5.0459e-01, -3.4546e-02,\n",
      "         -3.1737e-01, -3.5729e-01,  2.8087e-01, -4.7962e-02, -1.7800e-01,\n",
      "         -5.1289e-01, -3.2962e-01, -1.4186e-01, -8.5734e-02,  6.3984e-02,\n",
      "         -3.9614e-01, -2.4416e-02,  6.2357e-02,  1.9654e-02, -2.6898e-01,\n",
      "         -4.5370e-01, -2.7816e-01, -1.9440e-01, -2.5541e-01, -1.3493e-01,\n",
      "         -3.7538e-01, -1.6366e-01, -3.6337e-01]], device='cuda:0')\n",
      "tensor([21, 26, 14, 12,  5,  3,  2,  6], device='cuda:0')\n",
      "torch.Size([16, 38])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#testing one cycle\n",
    "model.eval()\n",
    "images, targets, target_lengths = next(testing_train_loader)\n",
    "\n",
    "images = images.to(device)\n",
    "targets = targets.to(device)\n",
    "target_lengths = target_lengths.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(images)\n",
    "print(output[0])\n",
    "print(targets[:target_lengths[0]])\n",
    "print(output[0].shape)\n",
    "print(output[0].argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ad5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 8, 6], device='cuda:0')\n",
      "torch.Size([4, 16, 38])\n",
      "tensor([[[-0.1104,  0.0869,  0.0212,  ..., -0.0262, -0.0195, -0.0979],\n",
      "         [-0.1081,  0.0991,  0.0240,  ..., -0.0231, -0.0275, -0.0977],\n",
      "         [-0.1203,  0.0975,  0.0440,  ..., -0.0397, -0.0223, -0.0958],\n",
      "         ...,\n",
      "         [-0.1156,  0.0892,  0.0679,  ..., -0.0024, -0.0121, -0.1438],\n",
      "         [-0.0961,  0.0845,  0.0551,  ...,  0.0138, -0.0075, -0.1401],\n",
      "         [-0.0660,  0.0660,  0.0314,  ...,  0.0603, -0.0014, -0.1513]],\n",
      "\n",
      "        [[-0.1094,  0.0639,  0.0084,  ...,  0.0041, -0.0238, -0.0621],\n",
      "         [-0.1015,  0.0572,  0.0246,  ...,  0.0041, -0.0236, -0.0793],\n",
      "         [-0.1047,  0.0594,  0.0490,  ...,  0.0114, -0.0097, -0.0903],\n",
      "         ...,\n",
      "         [-0.0730,  0.1294,  0.0465,  ...,  0.0418,  0.0877, -0.1646],\n",
      "         [-0.0652,  0.1371,  0.0137,  ...,  0.0632,  0.0934, -0.1779],\n",
      "         [-0.0556,  0.1084, -0.0020,  ...,  0.1112,  0.0581, -0.1776]],\n",
      "\n",
      "        [[-0.1094,  0.0972,  0.0148,  ..., -0.0086, -0.0073, -0.0947],\n",
      "         [-0.1075,  0.0973,  0.0259,  ...,  0.0078, -0.0074, -0.1122],\n",
      "         [-0.0999,  0.0926,  0.0364,  ...,  0.0202, -0.0039, -0.1342],\n",
      "         ...,\n",
      "         [-0.0749,  0.1152,  0.0264,  ..., -0.0021, -0.0131, -0.1484],\n",
      "         [-0.0803,  0.0918,  0.0171,  ...,  0.0252, -0.0169, -0.1577],\n",
      "         [-0.0759,  0.0674,  0.0034,  ...,  0.0700, -0.0264, -0.1490]],\n",
      "\n",
      "        [[-0.0588,  0.0875,  0.0188,  ..., -0.0359,  0.0043, -0.1233],\n",
      "         [-0.0879,  0.0755,  0.0244,  ..., -0.0138, -0.0133, -0.1233],\n",
      "         [-0.1042,  0.0468,  0.0428,  ...,  0.0008, -0.0519, -0.1334],\n",
      "         ...,\n",
      "         [-0.0656,  0.1344,  0.0610,  ...,  0.0151, -0.0280, -0.1434],\n",
      "         [-0.0625,  0.1191,  0.0392,  ...,  0.0271, -0.0069, -0.1341],\n",
      "         [-0.0507,  0.0807,  0.0107,  ...,  0.0644, -0.0221, -0.1502]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([16, 4, 38])\n",
      "tensor([16, 16, 16, 16], device='cuda:0')\n",
      "tensor([7, 0, 8, 6], device='cuda:0')\n",
      "Loss: 19.7619\n"
     ]
    }
   ],
   "source": [
    "#testing one cycle\n",
    "model.train()\n",
    "images, targets, target_lengths = next(testing_train_loader)\n",
    "\n",
    "images = images.to(device)\n",
    "targets = targets.to(device)\n",
    "target_lengths = target_lengths.to(device)\n",
    "print(target_lengths)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = model(images) # [B, T, C]\n",
    "print(outputs.shape)\n",
    "print(outputs)\n",
    "log_probability = outputs.permute(1, 0, 2) # [T, B, C]\n",
    "print(log_probability.shape)\n",
    "\n",
    "log_probability = F.log_softmax(log_probability, dim=2)\n",
    "\n",
    "# Input lengths: full length for each sequence\n",
    "input_lengths = torch.full(\n",
    "    size=(outputs.size(0),),       # B\n",
    "    fill_value=outputs.size(1),    # T\n",
    "    dtype=torch.long\n",
    ").to(device)\n",
    "print(input_lengths)\n",
    "print(target_lengths)\n",
    "assert all(input_lengths >= target_lengths), \"Target sequence too long for CTC\"\n",
    "\n",
    "# Loss computation\n",
    "loss = ctc_loss(log_probability, targets, input_lengths, target_lengths)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward and optimize\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e17ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5,     0] loss/50 = 0.1612 | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([34, 32, 14, 32, 20, 20, 21, 31], device='cuda:0')\n",
      "tensor([11, 11, 11, 25, 11, 27, 11, 11, 11, 25, 25, 25, 25, 25, 25, 25],\n",
      "       device='cuda:0')\n",
      "[1/5,    50] loss/50 = 5.6547 | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([21, 26,  2, 32, 13, 21, 13], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "[1/5,   100] loss/50 = 4.0558 | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([ 9, 28, 32,  4, 14,  6, 22], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "[1/5,   150] loss/50 = nan | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([ 9,  9,  9, 29, 29, 29], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "[1/5,   200] loss/50 = nan | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([ 6,  5,  4, 10, 10, 19, 12], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Skipping batch: target length exceeds input length.\n",
      "Skipping batch: target length exceeds input length.\n",
      "[1/5,   250] loss/50 = nan | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([ 3,  4,  5,  5, 10, 13, 30], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "[1/5,   300] loss/50 = nan | LR = 1.00e-04\n",
      "target_length: torch.Size([4]) input length: torch.Size([4])\n",
      "tensor([22, 22, 24, 11,  2,  4, 21], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#print(target_lengths)\u001b[39;00m\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B, T, C]\u001b[39;00m\n\u001b[32m     20\u001b[39m log_probability = F.log_softmax(outputs, dim=\u001b[32m2\u001b[39m)\n\u001b[32m     21\u001b[39m log_probability = log_probability.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m) \u001b[38;5;66;03m# [T, B, C]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sammu\\anaconda3\\envs\\torchCuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sammu\\anaconda3\\envs\\torchCuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mplate_OCR.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m) \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#x = self.dimension_reduction(x)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m rnn, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m result = \u001b[38;5;28mself\u001b[39m.classify(rnn)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sammu\\anaconda3\\envs\\torchCuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sammu\\anaconda3\\envs\\torchCuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sammu\\anaconda3\\envs\\torchCuda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1086\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1084\u001b[39m unsorted_indices = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m     h_zeros = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreal_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1094\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1095\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     hx = (h_zeros, c_zeros)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = plate_OCR().to(device)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss   = 0.0\n",
    "    i = 0\n",
    "\n",
    "    for images, targets, target_lengths in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        #print(target_lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # [B, T, C]\n",
    "        log_probability = F.log_softmax(outputs, dim=2)\n",
    "        log_probability = log_probability.permute(1, 0, 2) # [T, B, C]\n",
    "\n",
    "        # Input lengths: full length for each sequence\n",
    "        input_lengths = torch.full(\n",
    "            size=(outputs.size(0),),       # B\n",
    "            fill_value=outputs.size(1),    # T\n",
    "            dtype=torch.long\n",
    "        ).to(device)\n",
    "        \n",
    "        if (target_lengths > input_lengths).any():\n",
    "            # batch is impossible for CTC → skip\n",
    "            print(\"Skipping batch: target length exceeds input length.\")\n",
    "            optimizer.zero_grad(set_to_none=True)  # cheap no-op\n",
    "            continue    \n",
    "\n",
    "        # Loss computation\n",
    "        loss = ctc_loss(log_probability, targets, input_lengths, target_lengths)\n",
    "        #print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_val = loss.item()\n",
    "        running_loss += loss_val\n",
    "        epoch_loss   += loss_val\n",
    "        \n",
    "        if i % 50 == 0 or i == len(train_loader):\n",
    "            print(f\"[{epoch+1}/{epochs}, {i:5d}] \"\n",
    "                f\"loss/50 = {running_loss/50:.4f} | \"\n",
    "                f\"LR = {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            print(targets[:target_lengths[0]])\n",
    "            print(outputs[0].argmax(dim=1))\n",
    "            running_loss = 0.0\n",
    "        i += 1\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} finished - avg loss: {epoch_loss/len(train_loader):.4f}\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), \"number_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1931a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = \"../data/license_numbers/test/images\"\n",
    "y_test = \"../data/license_numbers/test/annotations.json\"\n",
    "\n",
    "test_dataset = license_coco(root=X_test, ann_file=y_test, transforms=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True, collate_fn=license_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b74fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.7804, -6.6093, -4.1751, -2.9140, -4.0386, -3.4436, -3.6946,\n",
      "          -3.2153, -5.9917, -8.0706, -4.5897, -3.7022, -3.5820, -2.5212,\n",
      "          -5.4768, -3.2019, -4.5439, -2.7529, -4.1720, -3.4432, -5.2258,\n",
      "          -2.1244, -3.5974, -3.7048, -6.8515, -3.9357, -4.8088, -3.9109,\n",
      "          -4.7454, -6.6749, -5.1487, -3.6975, -5.0427, -4.3240, -3.9655,\n",
      "          -4.4039, -1.5948, -6.9093, -4.5697, -7.2452],\n",
      "         [-7.1760, -6.7530, -4.0604, -3.7520, -4.2617, -3.0541, -3.8179,\n",
      "          -3.6107, -5.9244, -8.2178, -4.4114, -3.3120, -3.8766, -2.7352,\n",
      "          -5.4026, -2.8435, -4.1236, -3.5076, -4.1026, -3.3576, -4.9164,\n",
      "          -2.4316, -3.5425, -3.8740, -7.1301, -4.0793, -5.2137, -4.6204,\n",
      "          -4.9785, -6.9158, -4.4501, -2.9742, -5.0943, -4.3976, -3.4198,\n",
      "          -5.0700, -1.8349, -7.1707, -4.4202, -7.2544],\n",
      "         [-7.4924, -7.0548, -4.5867, -4.2835, -4.3063, -3.0697, -4.1318,\n",
      "          -4.0297, -5.9549, -8.4112, -4.2952, -3.6074, -4.3075, -3.0530,\n",
      "          -5.3680, -3.4583, -4.5827, -3.3679, -4.2062, -3.4349, -4.8850,\n",
      "          -3.1166, -3.7850, -4.2174, -7.4027, -3.9813, -5.6731, -5.0906,\n",
      "          -5.2138, -7.2897, -4.1790, -3.0675, -5.4685, -4.6290, -3.5337,\n",
      "          -5.5082, -1.4075, -7.4274, -4.5123, -7.4799],\n",
      "         [-7.9186, -7.4848, -5.2723, -4.7319, -4.4247, -3.5566, -4.5078,\n",
      "          -4.6446, -5.5397, -8.4262, -3.8842, -4.7314, -4.8607, -4.1161,\n",
      "          -5.2230, -4.4551, -5.5960, -2.2699, -4.5883, -3.6238, -5.0222,\n",
      "          -4.1519, -4.3628, -4.8364, -7.4843, -3.6538, -5.8385, -5.7576,\n",
      "          -5.4043, -7.4192, -3.5554, -3.5965, -5.8680, -4.9730, -4.2299,\n",
      "          -6.0549, -0.7795, -7.8190, -4.6414, -7.7813]],\n",
      "\n",
      "        [[-7.0182, -7.0656, -3.8367, -3.6296, -3.1210, -3.1892, -4.0669,\n",
      "          -3.1906, -5.2843, -7.4959, -3.6135, -3.1994, -3.1629, -3.8680,\n",
      "          -5.1881, -3.2686, -4.4370, -2.9362, -4.6826, -3.7727, -4.7631,\n",
      "          -3.0326, -3.0727, -4.6793, -6.7611, -3.8244, -4.5534, -4.1120,\n",
      "          -3.9261, -7.1895, -4.7600, -4.7175, -4.7193, -4.2501, -4.6917,\n",
      "          -4.2977, -2.2645, -6.9406, -4.4198, -7.2795],\n",
      "         [-6.5571, -6.3250, -2.8470, -4.0162, -2.7847, -2.1368, -3.5531,\n",
      "          -2.6948, -4.0191, -6.6668, -2.7953, -1.9923, -2.6927, -3.6229,\n",
      "          -4.2047, -1.8380, -2.9972, -4.1086, -4.1683, -3.3890, -3.4161,\n",
      "          -3.1779, -2.5038, -4.4487, -6.1477, -3.7875, -4.5463, -3.9079,\n",
      "          -3.4450, -6.4977, -3.3080, -3.3488, -3.9171, -3.4819, -3.6148,\n",
      "          -4.2841, -3.1905, -6.2203, -3.5911, -6.3373],\n",
      "         [-6.9224, -6.4630, -3.0772, -4.1723, -2.8346, -2.0667, -3.5752,\n",
      "          -2.7939, -3.5416, -6.7842, -2.2880, -2.4974, -3.0461, -3.9449,\n",
      "          -4.0622, -2.3208, -3.4441, -4.0743, -4.2028, -3.5376, -3.1025,\n",
      "          -3.7360, -2.7924, -4.5977, -6.2745, -3.8137, -4.9604, -4.0866,\n",
      "          -3.6513, -6.7010, -3.0261, -3.3496, -4.2645, -3.5908, -3.6286,\n",
      "          -4.6713, -3.0628, -6.3158, -3.8330, -6.5077],\n",
      "         [-6.9986, -6.3987, -3.7279, -4.2021, -3.1891, -2.4409, -3.6745,\n",
      "          -3.2838, -2.9449, -6.6556, -1.8032, -3.7056, -3.5872, -4.4888,\n",
      "          -3.9485, -3.2453, -4.2304, -3.0735, -4.2532, -3.3199, -3.1415,\n",
      "          -4.3001, -3.4809, -4.7369, -6.2533, -3.4704, -4.9702, -4.6189,\n",
      "          -3.9898, -6.4644, -2.4731, -3.4349, -4.6309, -3.9370, -3.6761,\n",
      "          -5.2255, -2.3569, -6.4787, -3.8273, -6.4702]],\n",
      "\n",
      "        [[-7.2596, -7.1873, -4.6217, -3.2048, -4.6152, -3.4563, -4.0319,\n",
      "          -3.4681, -6.5722, -8.2677, -4.6835, -4.1312, -4.2736, -2.7330,\n",
      "          -6.0446, -4.3701, -5.0214, -2.3298, -4.2202, -3.5179, -5.4976,\n",
      "          -2.7865, -4.2208, -3.9365, -7.4539, -4.2701, -5.4306, -4.3492,\n",
      "          -5.1739, -7.3890, -5.6146, -4.3990, -5.4304, -4.9792, -4.0938,\n",
      "          -4.9334, -1.1043, -7.3024, -4.8858, -7.6558],\n",
      "         [-6.9862, -6.8635, -4.2304, -3.5365, -4.3087, -2.6144, -3.6408,\n",
      "          -3.4177, -6.1114, -7.9777, -4.2063, -3.1997, -4.1237, -2.3490,\n",
      "          -5.5566, -3.7175, -4.2521, -3.3662, -3.7674, -3.3072, -4.8546,\n",
      "          -2.9022, -3.8987, -3.7715, -7.2263, -4.1367, -5.3762, -4.4309,\n",
      "          -5.0437, -7.1543, -4.7270, -3.5263, -5.1377, -4.7818, -3.1520,\n",
      "          -5.1052, -1.5514, -6.9978, -4.3402, -7.2173],\n",
      "         [-7.5946, -7.4476, -4.9615, -4.1674, -4.5459, -2.9327, -4.2726,\n",
      "          -4.1483, -6.2921, -8.6264, -4.3163, -4.0289, -4.6657, -2.9471,\n",
      "          -5.8539, -4.4789, -5.2380, -3.1458, -4.0717, -3.5357, -5.1958,\n",
      "          -3.6079, -4.3419, -4.4461, -7.7118, -4.1588, -6.0450, -5.1479,\n",
      "          -5.5150, -7.6729, -4.6792, -3.9070, -5.7653, -5.2344, -3.4971,\n",
      "          -5.7417, -0.9893, -7.6305, -4.7233, -7.8389],\n",
      "         [-7.6678, -7.4491, -5.2213, -4.6862, -4.5112, -3.4738, -4.5766,\n",
      "          -4.6987, -5.6591, -8.3226, -3.8661, -4.7997, -4.9659, -3.9476,\n",
      "          -5.5166, -4.6117, -5.7554, -2.2026, -4.2645, -3.4476, -5.1715,\n",
      "          -4.0975, -4.4785, -4.8892, -7.4454, -3.6870, -5.8228, -5.7687,\n",
      "          -5.4333, -7.3705, -3.8061, -3.7301, -5.8002, -5.0835, -3.8497,\n",
      "          -6.1111, -0.5341, -7.6481, -4.5485, -7.6165]],\n",
      "\n",
      "        [[-6.6028, -6.7101, -3.9514, -3.1368, -3.1182, -3.1824, -3.6723,\n",
      "          -3.2221, -5.2821, -7.3780, -4.0347, -2.7660, -2.5143, -3.5438,\n",
      "          -4.9874, -2.6872, -4.0287, -3.1489, -4.5099, -3.5287, -4.9914,\n",
      "          -2.4630, -2.9729, -4.6263, -6.3331, -3.5575, -4.0400, -3.8412,\n",
      "          -3.7664, -6.8439, -4.5964, -4.5105, -4.6733, -3.8678, -4.4501,\n",
      "          -4.0954, -2.4797, -6.7416, -3.8475, -7.0580],\n",
      "         [-6.7234, -6.3904, -3.2878, -3.9273, -3.0900, -2.5562, -3.6343,\n",
      "          -3.3066, -4.6119, -7.0614, -3.5526, -2.0480, -2.4468, -3.7214,\n",
      "          -4.5055, -1.5973, -2.9822, -4.0009, -4.3741, -3.2348, -4.2069,\n",
      "          -2.8357, -2.6855, -4.7446, -6.4199, -3.8353, -4.2127, -4.2058,\n",
      "          -3.7689, -6.6670, -3.6917, -3.5155, -4.3833, -3.5628, -3.5882,\n",
      "          -4.6359, -3.1206, -6.5667, -3.3410, -6.7935],\n",
      "         [-7.1997, -6.7189, -3.5378, -4.3940, -3.2248, -2.6608, -3.8802,\n",
      "          -3.6143, -4.2107, -7.3502, -3.2840, -2.5996, -2.7488, -4.2815,\n",
      "          -4.4465, -1.9752, -3.4720, -3.9468, -4.5599, -3.4723, -3.9860,\n",
      "          -3.4890, -3.1082, -5.1599, -6.7549, -4.0363, -4.5989, -4.6345,\n",
      "          -3.9230, -7.0676, -3.6802, -3.6548, -4.8695, -3.8178, -3.7851,\n",
      "          -5.1826, -3.1847, -6.8073, -3.5673, -7.1099],\n",
      "         [-6.8527, -6.2738, -3.7836, -4.2901, -3.2267, -2.5479, -3.7074,\n",
      "          -3.6265, -3.0431, -6.6639, -2.3944, -3.3559, -3.0209, -4.4654,\n",
      "          -3.7966, -2.6803, -3.9226, -3.1167, -4.2464, -3.2428, -3.3915,\n",
      "          -3.9500, -3.5252, -4.9006, -6.2652, -3.5951, -4.5790, -4.7798,\n",
      "          -3.9120, -6.5054, -2.8127, -3.3807, -4.8425, -3.7654, -3.6503,\n",
      "          -5.3145, -2.6635, -6.4595, -3.4293, -6.5161]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images, targets, target_lengths = next(iter(train_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "targets = targets.to(device)\n",
    "target_lengths = target_lengths.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(images)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d7a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
