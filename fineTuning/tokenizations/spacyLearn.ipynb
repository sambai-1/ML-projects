{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b73b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "# some powerful stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0728a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5633eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9829eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# rest is from https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_preprocessing.ipynb#scrollTo=BIoEJZ-IkHQ4\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a07aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "He didn't\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "He didn't want to pay $20 for this book.\n"
     ]
    }
   ],
   "source": [
    "print(doc[0])\n",
    "print(type(doc[0]))\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ab332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Either the well was very deep, or she fell very slowly, for she \n",
      "had plenty of time as she went down to look about her and to wonder what \n",
      "was going to happen next., First, she tried to look down and make out what \n",
      "she was coming to, but it was too dark to see anything; then she looked at \n",
      "the sides of the well, and noticed that they were filled with cupboards and \n",
      "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Either the well was very deep, or she fell very slowly, for she \n",
    "had plenty of time as she went down to look about her and to wonder what \n",
    "was going to happen next. First, she tried to look down and make out what \n",
    "she was coming to, but it was too dark to see anything; then she looked at \n",
    "the sides of the well, and noticed that they were filled with cupboards and \n",
    "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "print(list(doc.sents))\n",
    "print(list(doc.sents)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0958ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for this book\n",
      "['$20']\n"
     ]
    }
   ],
   "source": [
    "# some excercizes\n",
    "\n",
    "# get all currencies\n",
    "# Expected output: \"$20\".\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)\n",
    "ans = [doc[i].text + doc[i + 1].text for i in range(len(doc) - 1) if doc[i].is_currency and doc[i + 1].like_num]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac81cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "# custom rules\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"...gimme...?\", [{\"ORTH\": \"...gimme...?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0085c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
